---
title: "Regression Methods Project"
format: html
editor: visual
---

## Introduction

1.  fr
2.  frf
3.  rf
4.  f
5.  f
6.  f
7.  f
8.  f

## Read the data

Here we explore the data:

The columns of data_agg are:

-   head_head = nbr de fois où la pièce a commencé head et est retombé head

-   tail_head = nbr de fois où la pièce a commencé tail et est retombé head

-   N_start_heads_up = nbr de lancers avec le head en start

-   N_start_tails_up = nbr de lancers avec le tail en start

-   person = personne ayant lancé la pièce

-   coin = type de coin lancée

Après transformation, on a df qui est deux fois plus long et sépare les head head et les tails-tails de data_agg puis coin et personne sont maintenant des variables catégorielles.

df1 regroupe nombre de head sur nombre de lancer pour chaque personne et piece

df2 regroupe nombre de tail sur nombre de lancer pour chaque personne et pièce

```{r}
#| label: "Read the data"
#| echo: false
#| message: false 
#| warning: false
#| error: false

library(ggplot2)
library(dplyr)
library(stringr)
library(SMPracticals)
library(lme4)
library(mgcv)
set.seed(12345)
path = "/Users/kalilbouhadra/MS_Statistics/MS1/Regression_Methods/Project/Regression_Project"

df <- read.csv("data-agg.csv",header=T)
n <- nrow(df)
df1 <- df[,-c(2,4)] # heads to heads
df2 <- df[,-c(1,3)] # tails to heads
df2[,1] <- df2[,2]-df2[,1] # tails to tails 
names(df1) <- names(df2) <- c("y","m","person","coin") 
start <- rep(c("heads","tails"),c(n,n))
df <- rbind(df1,df2)
df$person <- factor(df$person) 
df$coin <- factor(df$coin)
df$start <- factor(start) 

head(df)
head(df1)
head(df2)
```

## 1. Exploratory Data Analysis

### 1.1. Empirical distribution

```{r}
#| label: "Global analysis"
#| echo: false
#| message: false 
#| warning: false
#| error: false


### Quick overview of the data
num_persons <- length(unique(df$person))
num_coins <- length(unique(df$coin))

cat("Number of persons:", num_persons, "\n")
cat("Number of coins:", num_coins, "\n")

### Histogram of proportions
df$success <- df$y / df$m
df_heads <- subset(df, start == "heads")
df_tails <- subset(df, start == "tails")

p_general <- ggplot(df, aes(x = success)) +
  geom_histogram(bins = 60, fill = "steelblue", color = "white", alpha = 0.8) +
  facet_wrap(~ start, ncol = 2) +
  labs(
    title = "Distribution of same-side proportions",
    x = "Same-side proportion",
    y = "Count",
  ) +
  theme_bw() + 
  theme(
    plot.title = element_text(hjust = 0.5),   
    plot.caption = element_text(hjust = 0.5),      
    legend.title = element_text(hjust = 0.5),   
    legend.text = element_text(hjust = 0.5)     
  )

# Compute statistics
stats <- df %>%
  group_by(start) %>%
  summarize(
    mu = mean(success, na.rm = TRUE),
    sigma = sd(success, na.rm = TRUE),
    .groups = "drop"
  )

p_general <- p_general + 
  geom_text(
    data = stats,
    aes(
      x = 0.68, 
      y = Inf, 
      label = paste0(
        "atop(hat(mu) == ", round(mu, 3), 
        ", hat(sigma) == ", round(sigma, 3), ")"
      )
    ),
    parse = TRUE,
    hjust = 1.1, vjust = 1.1, 
    size = 3,
    color = "black",
    inherit.aes = FALSE
  )

print(p_general)

ggsave(
  filename = "distribution_success.pdf",  
  plot = p_general,  
  device = "pdf", 
  path = path, 
  width = 8,
  height = 5
)
```

### 1.2. Outlier Analysis

```{r}
#| label: "Outliers Analysis"
#| echo: false
#| message: false 
#| warning: false
#| error: false


### (a) Summary of total flips and success by person
person_summary <- df %>%
  group_by(person) %>%
  summarize(
    total_flips = sum(m),
    mean_success = round(mean(success, na.rm = TRUE), 3),
    .groups = "drop"
  ) %>%
  arrange(mean_success)

print(person_summary)


### (b) Summary of total flips and success by coin
coin_summary <- df %>%
  group_by(coin) %>%
  summarize(
    total_flips = sum(m),
    mean_success = round(mean(success, na.rm = TRUE), 3),
    .groups = "drop"
  ) %>%
  arrange(mean_success)

print(coin_summary)


### (c) Summary of total flips and success by person-coin

person_coin_summary <- df %>%
  group_by(person, coin) %>%
  summarize(
    total_flips = sum(m),
    mean_success = round(mean(success, na.rm = TRUE), 3),
    .groups = "drop"
  ) %>%
  arrange(mean_success)  

print(person_coin_summary)


# Group the data
violin_data <- data.frame(
  category = factor(c(rep("by person", nrow(person_summary)),
                      rep("by coin", nrow(coin_summary)),
                      rep("by person x coin", nrow(person_coin_summary)))),
  mean_success = c(person_summary$mean_success,
                   coin_summary$mean_success,
                   person_coin_summary$mean_success)
)

# Plot
p_violin <- ggplot(violin_data, aes(x = "", y = mean_success)) +
  geom_violin(fill = "steelblue", color = "white", alpha = 0.8) +
  geom_boxplot(width = 0.1, fill = "lightgrey", color = "black", outlier.color = "red") +
  facet_wrap(~ category) +
  labs(
    title = "Distribution of Success Proportions",
    x = "",
    y = "Success Proportion",
  ) +
  theme_bw() + 
  theme(
    plot.title = element_text(hjust = 0.5),   
    plot.caption = element_text(hjust = 0.5),
    legend.title = element_text(hjust = 0.5),   
    legend.text = element_text(hjust = 0.5),
    strip.background = element_rect(fill = "grey90"),
    strip.text = element_text(face = "bold")
  )

print(p_violin)

ggsave(
  filename = "violin_plot_success.pdf",  
  plot = p_violin,  
  device = "pdf", 
  path = path, 
  width = 8,
  height = 5
)
```

### 1.3. Temporal effects

```{r}
#| label: "Temporal analysis"
#| echo: false
#| message: false 
#| warning: false
#| error: false

df_time <- read.csv("df-time-agg.csv", header = TRUE)
df_time <- df_time[, c("same_side", "agg")]

# Calcul de la moyenne des success proportions pour chaque "agg"
df_time <- df_time %>%
  group_by(agg) %>%
  summarize(mean_same_side = mean(same_side, na.rm = TRUE)) %>%
  ungroup()

# Transformer les résultats de l'ACF en DataFrame
acf_result <- acf(df_time$mean_same_side, plot = FALSE)
acf_df <- data.frame(
  lag = acf_result$lag,
  acf = acf_result$acf
)

# Plot de l'ACF en ggplot
p_acf <- ggplot(acf_df, aes(x = lag, y = acf)) +
  geom_bar(stat = "identity", fill = "steelblue", color = "white", alpha = 0.8) +  # Barres d'autocorrélation
  geom_hline(yintercept = 0, color = "black") +  # Ligne de référence à 0
  geom_hline(yintercept = c(-1.96 / sqrt(nrow(df_time)), 1.96 / sqrt(nrow(df_time))), linetype = "dashed", color = "red") +  # Bornes de significativité
  labs(
    title = "Autocorrelation Function (ACF) of the success proportion",
    x = "Lag",
    y = "ACF"
  ) +
  theme_bw() +
  theme(
    plot.title = element_text(hjust = 0.5),
    legend.title = element_text(hjust = 0.5),
    legend.text = element_text(hjust = 0.5),
    strip.background = element_rect(fill = "grey90"),
    strip.text = element_text(face = "bold")
  )

# Affichage du plot
print(p_acf)

ggsave(
  filename = "acf_success.pdf",  
  plot = p_acf,  
  device = "pdf", 
  path = path, 
  width = 8,
  height = 5
)
```

## 2. GLM Binomial

### 2.1. Base model

```{r}
#| label: "GLM Binomial base"
#| echo: false
#| message: false 
#| warning: false
#| error: false

# Ajustement du modèle GLM binomial simple
base_glm <- glm(cbind(y, m - y) ~ 1, family = binomial, data = df)

# Résumé du modèle pour voir les résultats
summary(base_glm)

(ci_logit_base <- confint(base_glm, level = 0.95))

# Transformation logit^-1 pour obtenir un IC sur p
ic_p_base <- exp(ci_logit_base) / (1 + exp(ci_logit_base))
cat("\n95% CI for p (base_glm): [",
    round(ic_p_base[1], 4), ", ",
    round(ic_p_base[2], 4), "]\n\n")

base_coef <- coef(base_glm)
p_initial <- exp(base_coef) / (1 + exp(base_coef))
cat("Estimated same-side probability (initial model):", round(p_initial, 4), "\n")


# Calcul du facteur de dispersion
dispersion_ratio <- summary(base_glm)$deviance / summary(base_glm)$df.residual
cat("Dispersion Ratio:", dispersion_ratio, "\n")

# Préparation des données pour le plot des résidus
df_resid <- data.frame(
  fitted = fitted(base_glm),
  pearson_res = residuals(base_glm, type = "pearson")
)



p_residuals <- ggplot(df_resid, aes(x = fitted, y = pearson_res)) +
  geom_point(color = "steelblue", alpha = 0.7) +
  geom_hline(yintercept = 0, color = "red", linetype = "dashed") +
  labs(
    title = "Residuals VS Fitted values",
    x = "Fitted values",
    y = "Pearson residuals"
  ) +
  theme_bw() +
  theme(
    plot.title = element_text(hjust = 0.5),
    legend.title = element_text(hjust = 0.5),
    legend.text = element_text(hjust = 0.5),
    strip.background = element_rect(fill = "grey90"),
    strip.text = element_text(face = "bold")
  )

print(p_residuals)

ggsave(
  filename = "p_residuals_baseline.pdf",  
  plot = p_residuals,  
  device = "pdf", 
  path = path, 
  width = 8,
  height = 5
)


df_resid$pearson_res <- residuals(base_glm, type = "pearson")

p_qq <- ggplot(df_resid, aes(sample = pearson_res)) +
  stat_qq(color = "steelblue", alpha = 0.7) +
  stat_qq_line(color = "red", linetype = "dashed") +
  labs(
    title = "Q-Q Plot of the Pearson residuals",
    x = "Residual quantiles",
    y = "Theoretical quantiles"
  ) +
  theme_bw() +
  theme(
    plot.title = element_text(hjust = 0.5),
  )

print(p_qq)

ggsave(
  filename = "qq_residuals.pdf",  
  plot = p_qq,  
  device = "pdf", 
  path = path, 
  width = 8,
  height = 5
)


# Calculer les quartiles et IQR
Q1 <- quantile(person_coin_summary$mean_success, 0.25)
Q3 <- quantile(person_coin_summary$mean_success, 0.75)
IQR_value <- Q3 - Q1

# Définir les seuils pour outliers
lower_bound <- Q1 - 1.5 * IQR_value
upper_bound <- Q3 + 1.5 * IQR_value

# Filtrer les observations considérées comme outliers
outliers_pc <- person_coin_summary %>%
  filter(mean_success < lower_bound | mean_success > upper_bound)

print(outliers_pc)

# Calculer les distances de Cook pour toutes les observations
cooks_d <- cooks.distance(base_glm)

# Ajouter la distance de Cook aux données d'origine
df_with_cook <- df
df_with_cook$cooks_distance <- cooks_d

# Extraire les combinaisons person-coin des outliers
outlier_groups <- outliers_pc %>% select(person, coin)

# Filtrer les données originales pour les observations appartenant à ces groupes
df_outliers <- df_with_cook %>%
  inner_join(outlier_groups, by = c("person", "coin"))

# Examiner les distances de Cook pour ces observations
df_outliers_selected <- df_outliers %>% select(person, coin, y, m, cooks_distance)

print(df_outliers_selected)

n <- nrow(df)

print(threshold <- 8/(n - 2*1))
# 0.01904 donc certain depasse ce seuil. Refit sans ces valeurs pour verifier si le resultat reste le meme 

# Identifier les indices des observations avec une distance de Cook > seuil
influential_indices <- which(cooks_d > threshold)

# Supprimer ces observations du jeu de données original
df_cleaned <- df[-influential_indices, ]

# Ajuster un nouveau modèle GLM binomial simple sur les données nettoyées
refit_glm <- glm(cbind(y, m - y) ~ 1, family = binomial, data = df_cleaned)

# Afficher le résumé du nouveau modèle
summary(refit_glm)

(ci_logit_refit <- confint(refit_glm, level = 0.95))
ic_p_refit <- exp(ci_logit_refit) / (1 + exp(ci_logit_refit))
cat("\n95% CI for p (refit_glm): [",
    round(ic_p_refit[1], 4), ", ",
    round(ic_p_refit[2], 4), "]\n\n")

refit_coef <- coef(refit_glm)
p_refit <- exp(refit_coef) / (1 + exp(refit_coef))
cat("Estimated same-side probability (model without influential points):", round(p_refit, 4), "\n")

```

### 2.2. Normal model

```{r}
#| label: "Normal linear model base"
#| echo: false
#| message: false 
#| warning: false
#| error: false

df$prop <- df$y / df$m

wls_model <- lm(prop ~ 1, data = df, weights = df$m)

#============================#
# 2) Extraction des paramètres
#============================#
beta0_hat <- coef(wls_model)[1]              # Estimateur de l'intercept
var_beta0_hat <- vcov(wls_model)[1, 1]       # Variance de l'intercept
se_beta0_hat <- sqrt(var_beta0_hat)          # Erreur standard

#============================#
# 3) Test H0 : beta0 = 0.5
#============================#
T_value <- (beta0_hat - 0.5) / se_beta0_hat
df_res   <- wls_model$df.residual

# p-value bilatérale (car H1: beta0 != 0.5)
p_value <- 2 * pt(abs(T_value), df_res, lower.tail = FALSE)

cat("\nTest H0: beta0 = 0.5 vs. H1: beta0 != 0.5 (WLS model)\n")
cat("---------------------------------------\n")
cat("Estimate beta0      =", round(beta0_hat, 4), "\n")
cat("Std Error           =", round(se_beta0_hat, 4), "\n")
cat("T-value             =", round(T_value, 4), "\n")
cat("Degrees of freedom  =", df_res, "\n")
cat("p-value (bilateral) =", format.pval(p_value, digits=4), "\n\n")


alpha <- 0.05
t_crit <- qt(1 - alpha/2, df_res)  # quantile t pour df_res

delta <- t_crit * se_beta0_hat
center <- beta0_hat
IC_left  <- center - delta
IC_right <- center + delta

cat("Manual 95% CI (around the estimate, testing 0.5) : [",
    round(IC_left, 4), ", ", round(IC_right, 4), "]\n\n")
```

### 2.3. Fixed effect GLM

```{r}
#| label: "GLM Binomial"
#| echo: false
#| message: false 
#| warning: false
#| error: false

# Définition des formules pour les différents modèles candidats
formulas <- list(
  "start + person + coin" = cbind(y, m - y) ~ start + person + coin,
  "start + person"       = cbind(y, m - y) ~ start + person,
  "start + coin"         = cbind(y, m - y) ~ start + coin,
  "person + coin"        = cbind(y, m - y) ~ person + coin,
  "start"                = cbind(y, m - y) ~ start,
  "person"               = cbind(y, m - y) ~ person,
  "coin"                 = cbind(y, m - y) ~ coin,
  "intercept"            = cbind(y, m - y) ~ 1
)

# Initialisation de listes pour stocker les modèles et leurs AIC
model_list <- list()
aic_values <- numeric(length(formulas))

# Ajustement de chaque modèle et calcul de son AIC
for(i in seq_along(formulas)){
  formula_i <- formulas[[i]]
  model_list[[i]] <- glm(formula_i, family = binomial, data = df)
  aic_values[i] <- AIC(model_list[[i]])
}

# Création d'une table récapitulative des modèles et leurs AIC
model_comparison <- data.frame(
  Model = names(formulas),
  AIC = aic_values
)

# Tri du tableau par AIC croissant
model_comparison <- model_comparison[order(model_comparison$AIC), ]
print(model_comparison)

# Identification du meilleur modèle selon l'AIC
best_model_index <- which.min(aic_values)
best_model_name <- model_comparison$Model[1]
best_model <- model_list[[best_model_index]]

cat("Meilleur modèle selon l'AIC :", best_model_name, "\n")
summary(best_model)

# Supposons que le meilleur modèle est celui avec effet "person" uniquement
best_model <- glm(cbind(y, m - y) ~ person, family = binomial, data = df)
summary(best_model)

# Calculer les résidus Pearson pour le modèle optimal
residuals_best <- residuals(best_model, type = "pearson")
fitted_best <- fitted(best_model)

# Préparer un data frame pour la visualisation des résidus
df_resid_best <- data.frame(fitted = fitted_best, pearson_res = residuals_best)

# Plot des résidus vs valeurs prédites
p_residuals_fixed_effect <- ggplot(df_resid_best, aes(x = fitted, y = pearson_res)) +
  geom_point(color = "steelblue", alpha = 0.7) +
  geom_hline(yintercept = 0, color = "red", linetype = "dashed") +
  labs(title = "Residuals VS Fitted values",
       x = "Fitted values", y = "Pearson residuals") +
  theme_bw() +
    theme(
    plot.title = element_text(hjust = 0.5),
    legend.title = element_text(hjust = 0.5),
    legend.text = element_text(hjust = 0.5),
    strip.background = element_rect(fill = "grey90"),
    strip.text = element_text(face = "bold")
  )

print(p_residuals_fixed_effect)

ggsave(
  filename = "p_residuals_fixed_effect.pdf",  
  plot = p_residuals_fixed_effect,  
  device = "pdf", 
  path = path, 
  width = 8,
  height = 5
)

# Calcul du facteur de dispersion
dispersion_ratio <- summary(best_model)$deviance / summary(best_model)$df.residual
cat("Dispersion Ratio:", dispersion_ratio, "\n")

quasi_glm <- glm(cbind(y, m - y) ~ person, family = quasibinomial, data = df)
summary(quasi_glm)

```

### 2.4. Random Effect GLM

```{r}
#| label: "Random Effect GLM"
#| echo: false
#| message: false 
#| warning: false
#| error: false


random_effect_person <- glmer(
  cbind(y, m - y) ~ 1 + (1|person), 
  family = binomial, 
  data = df
)

summary(random_effect_person)

dispersion_ratio <- summary(random_effect_person)$deviance / summary(random_effect_person)$df.residual
cat("Dispersion Ratio:", dispersion_ratio, "\n")

random_effect_person_nout <- glmer(
  cbind(y, m - y) ~ 1 + (1 | person),
  family = binomial,
  data = df_cleaned
)


```

### 2.5. Nested Effect GLM

```{r}
#| label: "Nested Effect GLM"
#| echo: false
#| message: false 
#| warning: false
#| error: false

re_model_nested <- glmer(
  cbind(y, m - y) ~ 1 + (1|person) + (1|person:coin),
  family = binomial,
  data = df
)

summary(re_model_nested)

anova(random_effect_person, re_model_nested, test="LRT")
AIC(random_effect_person, re_model_nested)

dispersion_ratio <- summary(re_model_nested)$deviance / summary(re_model_nested)$df.residual
cat("Dispersion Ratio:", dispersion_ratio, "\n")


re_model_nested_nout <- glmer(
  cbind(y, m - y) ~ 1 + (1 | person) + (1 | person : coin),
  family = binomial,
  data = df_cleaned
)
summary(re_model_nested_nout)

AIC(re_model_nested_nout)

anova(random_effect_person_nout, re_model_nested_nout, test="LRT")



```

### 2.6. Learning Effect - Time

```{r}
#| label: "Learning Effect"
#| echo: false
#| message: false 
#| warning: false
#| error: false


df_time_agg <- read.csv("df-time-agg.csv", header=T, stringsAsFactors = FALSE)

linear_learning <- glm(
  cbind(same_side, N - same_side) ~ mean_toss,
  family = binomial,
  data   = df_time_agg
)

summary(linear_learning)


spline_learning <- gam(
  cbind(same_side, N - same_side) ~ s(mean_toss),
  family = binomial,
  data   = df_time_agg
)

summary(spline_learning)

AIC(linear_learning, spline_learning)


# Create a sequence of mean_toss values spanning the observed range
new_data <- data.frame(mean_toss = seq(min(df_time_agg$mean_toss), 
                                       max(df_time_agg$mean_toss), 
                                       length.out = 200))

# Obtain predictions on the link (logit) scale along with standard errors
pred <- predict(spline_learning, newdata = new_data, type = "link", se.fit = TRUE)

# Inverse link function for binomial logit
linkinv <- spline_learning$family$linkinv

# Transform predictions to the response scale (probabilities)
new_data$fit <- linkinv(pred$fit)

# Compute approximate 95% confidence intervals on the response scale
new_data$lwr <- linkinv(pred$fit - 2 * pred$se.fit)
new_data$upr <- linkinv(pred$fit + 2 * pred$se.fit)

# Plot the estimated smooth function with confidence ribbon using ggplot2
library(ggplot2)

p_spline <- ggplot(new_data, aes(x = mean_toss, y = fit)) +
  geom_line(color = "steelblue", size = 1) +                          # Estimated probability line
  geom_ribbon(aes(ymin = lwr, ymax = upr), fill = "steelblue", alpha = 0.2) +  # Confidence interval
  labs(
    title = "Estimated Learning Effect over Time",
    x = "Mean Toss",
    y = "Estimated Same-Side Probability"
  ) +
  theme_bw() +
  theme(
    plot.title = element_text(hjust = 0.5),
    axis.title = element_text(face = "bold")
  )

print(p_spline)

# Save the plot as a PDF
ggsave(
  filename = "learning_spline_plot.pdf",  
  plot = p_spline,  
  device = "pdf", 
  path = path,
  width = 8,
  height = 5
)
```

### 2.7. Previous flip impact

```{r}
#| label: "Previous flip impact"
#| echo: false
#| message: false 
#| warning: false
#| error: false


# Charger les données agrégées
df_time_agg <- read.csv("df-time-agg.csv", header = TRUE, stringsAsFactors = FALSE)

# Trier par personne, pièce et ordre de bloc pour assurer un bon alignement temporel
df_time_agg <- df_time_agg %>%
  arrange(person, coin, agg)

# Créer une variable avec la proportion de same_side du bloc précédent pour chaque observation
df_time_agg <- df_time_agg %>%
  group_by(person, coin) %>%
  mutate(prev_prop_same_side = lag(same_side / N)) %>%
  ungroup()

# Filtrer les premières lignes de chaque groupe où prev_prop_same_side est NA
df_time_agg_filtered <- df_time_agg %>% filter(!is.na(prev_prop_same_side))

# Ajuster le modèle logistique avec l'effet du bloc précédent
model_recent_block <- glm(
  cbind(same_side, N - same_side) ~ prev_prop_same_side,
  family = binomial,
  data = df_time_agg_filtered
)

summary(model_recent_block)


```
