---
title: "Regression Methods Project"
format: html
editor: visual
---

## Introduction

1.  fr
2.  frf
3.  rf
4.  f
5.  f
6.  f
7.  f
8.  f

## Read the data

Here we explore the data:

The columns of data_agg are:

-   head_head = nbr de fois où la pièce a commencé head et est retombé head

-   tail_head = nbr de fois où la pièce a commencé tail et est retombé head

-   N_start_heads_up = nbr de lancers avec le head en start

-   N_start_tails_up = nbr de lancers avec le tail en start

-   person = personne ayant lancé la pièce

-   coin = type de coin lancée

Après transformation, on a df qui est deux fois plus long et sépare les head head et les tails-tails de data_agg puis coin et personne sont maintenant des variables catégorielles.

df1 regroupe nombre de head sur nombre de lancer pour chaque personne et piece

df2 regroupe nombre de tail sur nombre de lancer pour chaque personne et pièce

```{r}
#| label: "Read the data"
#| echo: false
#| message: false 
#| warning: false
#| error: false

library(ggplot2)
library(dplyr)
library(stringr)
library(SMPracticals)
set.seed(12345)
path = "/Users/kalilbouhadra/MS_Statistics/MS1/Regression_Methods/Project/Regression_Project"

df <- read.csv("data-agg.csv",header=T)
n <- nrow(df)
df1 <- df[,-c(2,4)] # heads to heads
df2 <- df[,-c(1,3)] # tails to heads
df2[,1] <- df2[,2]-df2[,1] # tails to tails 
names(df1) <- names(df2) <- c("y","m","person","coin") 
start <- rep(c("heads","tails"),c(n,n))
df <- rbind(df1,df2)
df$person <- factor(df$person) 
df$coin <- factor(df$coin)
df$start <- factor(start) 

head(df)
head(df1)
head(df2)
```

## 1. Exploratory Data Analysis

### 1.1. Empirical distribution

```{r}
#| label: "Global analysis"
#| echo: false
#| message: false 
#| warning: false
#| error: false


### Quick overview of the data
num_persons <- length(unique(df$person))
num_coins <- length(unique(df$coin))

cat("Number of persons:", num_persons, "\n")
cat("Number of coins:", num_coins, "\n")

### Histogram of proportions
df$success <- df$y / df$m
df_heads <- subset(df, start == "heads")
df_tails <- subset(df, start == "tails")

p_general <- ggplot(df, aes(x = success)) +
  geom_histogram(bins = 60, fill = "steelblue", color = "white", alpha = 0.8) +
  facet_wrap(~ start, ncol = 2) +
  labs(
    title = "Distribution of same-side proportions",
    x = "Same-side proportion",
    y = "Count",
  ) +
  theme_bw() + 
  theme(
    plot.title = element_text(hjust = 0.5),   
    plot.caption = element_text(hjust = 0.5),      
    legend.title = element_text(hjust = 0.5),   
    legend.text = element_text(hjust = 0.5)     
  )

# Compute statistics
stats <- df %>%
  group_by(start) %>%
  summarize(
    mu = mean(success, na.rm = TRUE),
    sigma = sd(success, na.rm = TRUE),
    .groups = "drop"
  )

p_general <- p_general + 
  geom_text(
    data = stats,
    aes(
      x = 0.68, 
      y = Inf, 
      label = paste0(
        "atop(hat(mu) == ", round(mu, 3), 
        ", hat(sigma) == ", round(sigma, 3), ")"
      )
    ),
    parse = TRUE,
    hjust = 1.1, vjust = 1.1, 
    size = 3,
    color = "black",
    inherit.aes = FALSE
  )

print(p_general)

ggsave(
  filename = "distribution_success.pdf",  
  plot = p_general,  
  device = "pdf", 
  path = path, 
  width = 8,
  height = 5
)
```

### 1.2. Outlier Analysis

```{r}
#| label: "Outliers Analysis"
#| echo: false
#| message: false 
#| warning: false
#| error: false


### (a) Summary of total flips and success by person
person_summary <- df %>%
  group_by(person) %>%
  summarize(
    total_flips = sum(m),
    mean_success = round(mean(success, na.rm = TRUE), 3),
    .groups = "drop"
  ) %>%
  arrange(mean_success)

print(person_summary)


### (b) Summary of total flips and success by coin
coin_summary <- df %>%
  group_by(coin) %>%
  summarize(
    total_flips = sum(m),
    mean_success = round(mean(success, na.rm = TRUE), 3),
    .groups = "drop"
  ) %>%
  arrange(mean_success)

print(coin_summary)


### (c) Summary of total flips and success by person-coin

person_coin_summary <- df %>%
  group_by(person, coin) %>%
  summarize(
    total_flips = sum(m),
    mean_success = round(mean(success, na.rm = TRUE), 3),
    .groups = "drop"
  ) %>%
  arrange(mean_success)  

print(person_coin_summary)


# Group the data
violin_data <- data.frame(
  category = factor(c(rep("by person", nrow(person_summary)),
                      rep("by coin", nrow(coin_summary)),
                      rep("by person x coin", nrow(person_coin_summary)))),
  mean_success = c(person_summary$mean_success,
                   coin_summary$mean_success,
                   person_coin_summary$mean_success)
)

# Plot
p_violin <- ggplot(violin_data, aes(x = "", y = mean_success)) +
  geom_violin(fill = "steelblue", color = "white", alpha = 0.8) +
  geom_boxplot(width = 0.1, fill = "lightgrey", color = "black", outlier.color = "red") +
  facet_wrap(~ category) +
  labs(
    title = "Distribution of Success Proportions",
    x = "",
    y = "Success Proportion",
  ) +
  theme_bw() + 
  theme(
    plot.title = element_text(hjust = 0.5),   
    plot.caption = element_text(hjust = 0.5),
    legend.title = element_text(hjust = 0.5),   
    legend.text = element_text(hjust = 0.5),
    strip.background = element_rect(fill = "grey90"),
    strip.text = element_text(face = "bold")
  )

print(p_violin)

ggsave(
  filename = "violin_plot_success.pdf",  
  plot = p_violin,  
  device = "pdf", 
  path = path, 
  width = 8,
  height = 5
)
```

### 1.3. Temporal effects

```{r}
#| label: "Temporal analysis"
#| echo: false
#| message: false 
#| warning: false
#| error: false


# Transformer les résultats de l'ACF en DataFrame
acf_result <- acf(df_time$mean_same_side, plot = FALSE)
acf_df <- data.frame(
  lag = acf_result$lag,
  acf = acf_result$acf
)

# Plot de l'ACF en ggplot
p_acf <- ggplot(acf_df, aes(x = lag, y = acf)) +
  geom_bar(stat = "identity", fill = "steelblue", color = "white", alpha = 0.8) +  # Barres d'autocorrélation
  geom_hline(yintercept = 0, color = "black") +  # Ligne de référence à 0
  geom_hline(yintercept = c(-1.96 / sqrt(nrow(df_time)), 1.96 / sqrt(nrow(df_time))), linetype = "dashed", color = "red") +  # Bornes de significativité
  labs(
    title = "Autocorrelation Function (ACF) of the success proportion",
    x = "Lag",
    y = "ACF"
  ) +
  theme_bw() +
  theme(
    plot.title = element_text(hjust = 0.5),
    legend.title = element_text(hjust = 0.5),
    legend.text = element_text(hjust = 0.5),
    strip.background = element_rect(fill = "grey90"),
    strip.text = element_text(face = "bold")
  )

# Affichage du plot
print(p_acf)

ggsave(
  filename = "acf_success.pdf",  
  plot = p_acf,  
  device = "pdf", 
  path = path, 
  width = 8,
  height = 5
)
```

## 2. GLM Binomial

### 2.1. Base model

```{r}
#| label: "GLM Binomial base"
#| echo: false
#| message: false 
#| warning: false
#| error: false

# Ajustement du modèle GLM binomial simple
base_glm <- glm(cbind(y, m - y) ~ 1, family = binomial, data = df)

# Résumé du modèle pour voir les résultats
summary(base_glm)

(ci_logit_base <- confint(base_glm, level = 0.95))

# Transformation logit^-1 pour obtenir un IC sur p
ic_p_base <- exp(ci_logit_base) / (1 + exp(ci_logit_base))
cat("\n95% CI for p (base_glm): [",
    round(ic_p_base[1], 4), ", ",
    round(ic_p_base[2], 4), "]\n\n")

base_coef <- coef(base_glm)
p_initial <- exp(base_coef) / (1 + exp(base_coef))
cat("Estimated same-side probability (initial model):", round(p_initial, 4), "\n")


# Calcul du facteur de dispersion
dispersion_ratio <- summary(base_glm)$deviance / summary(base_glm)$df.residual
cat("Dispersion Ratio:", dispersion_ratio, "\n")

# Préparation des données pour le plot des résidus
df_resid <- data.frame(
  fitted = fitted(base_glm),
  pearson_res = residuals(base_glm, type = "pearson")
)

caption_resid <- str_wrap("Figure X: Résidus Pearson vs. Valeurs Prévues du modèle GLM Binomial simple.", width = 85)

p_residuals <- ggplot(df_resid, aes(x = fitted, y = pearson_res)) +
  geom_point(color = "steelblue", alpha = 0.7) +
  geom_hline(yintercept = 0, color = "red", linetype = "dashed") +
  labs(
    title = "Résidus vs Valeurs Prévues",
    x = "Valeurs Prévues",
    y = "Résidus Pearson",
    caption = caption_resid
  ) +
  theme_bw() +
  theme(
    plot.title = element_text(hjust = 0.5),
    plot.caption = element_text(hjust = 0.5)
  )

print(p_residuals)


df_resid$pearson_res <- residuals(base_glm, type = "pearson")

caption_qq <- str_wrap("Figure Y: Q-Q Plot des Résidus Pearson du modèle GLM Binomial simple.", width = 85)

p_qq <- ggplot(df_resid, aes(sample = pearson_res)) +
  stat_qq(color = "steelblue", alpha = 0.7) +
  stat_qq_line(color = "red", linetype = "dashed") +
  labs(
    title = "Q-Q Plot des Résidus Pearson",
    x = "Quantiles théoriques",
    y = "Quantiles des résidus",
    caption = caption_qq
  ) +
  theme_bw() +
  theme(
    plot.title = element_text(hjust = 0.5),
    plot.caption = element_text(hjust = 0.5)
  )

print(p_qq)


# Calculer les quartiles et IQR
Q1 <- quantile(person_coin_summary$mean_success, 0.25)
Q3 <- quantile(person_coin_summary$mean_success, 0.75)
IQR_value <- Q3 - Q1

# Définir les seuils pour outliers
lower_bound <- Q1 - 1.5 * IQR_value
upper_bound <- Q3 + 1.5 * IQR_value

# Filtrer les observations considérées comme outliers
outliers_pc <- person_coin_summary %>%
  filter(mean_success < lower_bound | mean_success > upper_bound)

print(outliers_pc)

# Calculer les distances de Cook pour toutes les observations
cooks_d <- cooks.distance(base_glm)

# Ajouter la distance de Cook aux données d'origine
df_with_cook <- df
df_with_cook$cooks_distance <- cooks_d

# Extraire les combinaisons person-coin des outliers
outlier_groups <- outliers_pc %>% select(person, coin)

# Filtrer les données originales pour les observations appartenant à ces groupes
df_outliers <- df_with_cook %>%
  inner_join(outlier_groups, by = c("person", "coin"))

# Examiner les distances de Cook pour ces observations
df_outliers_selected <- df_outliers %>% select(person, coin, y, m, cooks_distance)

print(df_outliers_selected)

n <- nrow(df)

print(threshold <- 8/(n - 2*1))
# 0.01904 donc certain depasse ce seuil. Refit sans ces valeurs pour verifier si le resultat reste le meme 

# Identifier les indices des observations avec une distance de Cook > seuil
influential_indices <- which(cooks_d > threshold)

# Supprimer ces observations du jeu de données original
df_cleaned <- df[-influential_indices, ]

# Ajuster un nouveau modèle GLM binomial simple sur les données nettoyées
refit_glm <- glm(cbind(y, m - y) ~ 1, family = binomial, data = df_cleaned)

# Afficher le résumé du nouveau modèle
summary(refit_glm)

(ci_logit_refit <- confint(refit_glm, level = 0.95))
ic_p_refit <- exp(ci_logit_refit) / (1 + exp(ci_logit_refit))
cat("\n95% CI for p (refit_glm): [",
    round(ic_p_refit[1], 4), ", ",
    round(ic_p_refit[2], 4), "]\n\n")

refit_coef <- coef(refit_glm)
p_refit <- exp(refit_coef) / (1 + exp(refit_coef))
cat("Estimated same-side probability (model without influential points):", round(p_refit, 4), "\n")

```

### 2.2. Normal model

```{r}
#| label: "Normal linear model base"
#| echo: false
#| message: false 
#| warning: false
#| error: false

# Création de la variable proportionnelle success
df$prop <- df$y / df$m

# Ajustement d'un modèle linéaire pondéré avec intercept seulement
# Poids = m (les constantes multiplicatives n'affectent pas les estimations)
wls_model <- lm(prop ~ 1, data = df, weights = df$m)

# Résumé du modèle WLS
summary(wls_model)

(confint_wls <- confint(wls_model, level = 0.95))
cat("\n95% CI for the mean proportion (WLS): [",
    round(confint_wls[1,1], 4), ", ",
    round(confint_wls[1,2], 4), "]\n\n")

# Calcul de la probabilité estimée depuis le modèle WLS
p_wls <- coef(wls_model)[1]
cat("Estimated same-side probability using WLS:", round(p_wls, 4), "\n")

# Ajuster un modèle gaussian avec poids pour reproduire WLS
glm_model_wls <- glm(prop ~ 1, family = gaussian, weights = m, data = df)
summary(glm_model_wls)
```

### 2.3. Fixed effect GLM

```{r}
#| label: "GLM Binomial"
#| echo: false
#| message: false 
#| warning: false
#| error: false

# Définition des formules pour les différents modèles candidats
formulas <- list(
  "start + person + coin" = cbind(y, m - y) ~ start + person + coin,
  "start + person"       = cbind(y, m - y) ~ start + person,
  "start + coin"         = cbind(y, m - y) ~ start + coin,
  "person + coin"        = cbind(y, m - y) ~ person + coin,
  "start"                = cbind(y, m - y) ~ start,
  "person"               = cbind(y, m - y) ~ person,
  "coin"                 = cbind(y, m - y) ~ coin,
  "intercept"            = cbind(y, m - y) ~ 1
)

# Initialisation de listes pour stocker les modèles et leurs AIC
model_list <- list()
aic_values <- numeric(length(formulas))

# Ajustement de chaque modèle et calcul de son AIC
for(i in seq_along(formulas)){
  formula_i <- formulas[[i]]
  model_list[[i]] <- glm(formula_i, family = binomial, data = df)
  aic_values[i] <- AIC(model_list[[i]])
}

# Création d'une table récapitulative des modèles et leurs AIC
model_comparison <- data.frame(
  Model = names(formulas),
  AIC = aic_values
)

# Tri du tableau par AIC croissant
model_comparison <- model_comparison[order(model_comparison$AIC), ]
print(model_comparison)

# Identification du meilleur modèle selon l'AIC
best_model_index <- which.min(aic_values)
best_model_name <- model_comparison$Model[1]
best_model <- model_list[[best_model_index]]

cat("Meilleur modèle selon l'AIC :", best_model_name, "\n")
summary(best_model)

# Supposons que le meilleur modèle est celui avec effet "person" uniquement
best_model <- glm(cbind(y, m - y) ~ person, family = binomial, data = df)
summary(best_model)

# Calculer les résidus Pearson pour le modèle optimal
residuals_best <- residuals(best_model, type = "pearson")
fitted_best <- fitted(best_model)

# Préparer un data frame pour la visualisation des résidus
df_resid_best <- data.frame(fitted = fitted_best, pearson_res = residuals_best)

# Plot des résidus vs valeurs prédites
ggplot(df_resid_best, aes(x = fitted, y = pearson_res)) +
  geom_point(color = "steelblue", alpha = 0.7) +
  geom_hline(yintercept = 0, color = "red", linetype = "dashed") +
  labs(title = "Résidus vs Valeurs Prévues (Modèle Optimal)",
       x = "Valeurs Prévues", y = "Résidus Pearson") +
  theme_bw()

# Calcul du facteur de dispersion
dispersion_ratio <- summary(best_model)$deviance / summary(best_model)$df.residual
cat("Dispersion Ratio:", dispersion_ratio, "\n")

quasi_glm <- glm(cbind(y, m - y) ~ person, family = quasibinomial, data = df)
summary(quasi_glm)

```
