---
title: "Regression Methods Project"
format: html
editor: visual
---

## Introduction



## Read the data

Here we explore the data:

The columns of data_agg are:

-   head_head = Number of times the coin lands on head; starting heads up

-   tail_head = Number of times the coin lands on tail; starting tails up

-   N_start_heads_up = Number of coin flips ; starting heads up

-   N_start_tails_up = Number of coin flips ; starting tails up

-   person = launcher 

-   coin = Kind of coin

Après transformation, on a df qui est deux fois plus long et sépare les head head et les tails-tails de data_agg puis coin et personne sont maintenant des variables catégorielles.

df1 regroupe nombre de head sur nombre de lancer pour chaque personne et piece

df2 regroupe nombre de tail sur nombre de lancer pour chaque personne et pièce

```{r}
#| label: "Read the data"
#| echo: false
#| message: false 
#| warning: false
#| error: false

library(ggplot2)
library(dplyr)
library(stringr)
library(SMPracticals)
library(lme4)
set.seed(12345)
path = "/Users/kalilbouhadra/MS_Statistics/MS1/Regression_Methods/Project/Regression_Project"

df <- read.csv("data-agg.csv",header=T)
n <- nrow(df)
df1 <- df[,-c(2,4)] # heads to heads
df2 <- df[,-c(1,3)] # tails to heads
df2[,1] <- df2[,2]-df2[,1] # tails to tails 
names(df1) <- names(df2) <- c("y","m","person","coin") 
start <- rep(c("heads","tails"),c(n,n))
df <- rbind(df1,df2)
df$person <- factor(df$person) 
df$coin <- factor(df$coin)
df$start <- factor(start) 

head(df)
head(df1)
head(df2)
```

# Load required libraries
library(ggplot2)
library(dplyr)
library(stringr)
library(SMPracticals)
library(lme4)

set.seed(12345)
path = "/Users/kalilbouhadra/MS_Statistics/MS1/Regression_Methods/Project/Regression_Project"

# Read the data
df <- read.csv("data-agg.csv", header = TRUE)
n <- nrow(df)

# Split the data into heads and tails transformations
df1 <- df[,-c(2,4)]  # Heads to heads
df2 <- df[,-c(1,3)]  # Tails to heads
df2[,1] <- df2[,2] - df2[,1]  # Tails to tails
names(df1) <- names(df2) <- c("y", "m", "person", "coin")

# Add start condition labels
start <- rep(c("heads", "tails"), c(n, n))
df <- rbind(df1, df2)
df$person <- factor(df$person)
df$coin <- factor(df$coin)
df$start <- factor(start)

# Display the first few rows of the data
head(df)
head(df1)
head(df2)

# ================================
# 1. Exploratory Data Analysis
# ================================

# 1.1. Empirical Distribution

# Quick overview of the dataset
num_persons <- length(unique(df$person))
num_coins <- length(unique(df$coin))
cat("Number of persons:", num_persons, "\n")
cat("Number of coins:", num_coins, "\n")

# Calculate success proportion
df$success <- df$y / df$m
df_heads <- subset(df, start == "heads")
df_tails <- subset(df, start == "tails")

# Plot the distribution of proportions
p_general <- ggplot(df, aes(x = success)) +
  geom_histogram(bins = 60, fill = "steelblue", color = "white", alpha = 0.8) +
  facet_wrap(~ start, ncol = 2) +
  labs(
    title = "Distribution of Same-Side Proportions",
    x = "Same-Side Proportion",
    y = "Count"
  ) +
  theme_bw() +
  theme(
    plot.title = element_text(hjust = 0.5),
    plot.caption = element_text(hjust = 0.5),
    legend.title = element_text(hjust = 0.5),
    legend.text = element_text(hjust = 0.5)
  )

# Compute and display statistics
stats <- df %>%
  group_by(start) %>%
  summarize(
    mu = mean(success, na.rm = TRUE),
    sigma = sd(success, na.rm = TRUE),
    .groups = "drop"
  )

p_general <- p_general + 
  geom_text(
    data = stats,
    aes(
      x = 0.68,
      y = Inf,
      label = paste0(
        "atop(hat(mu) == ", round(mu, 3),
        ", hat(sigma) == ", round(sigma, 3), ")"
      )
    ),
    parse = TRUE,
    hjust = 1.1, vjust = 1.1,
    size = 3,
    color = "black",
    inherit.aes = FALSE
  )

print(p_general)

# Save the plot
ggsave(
  filename = "distribution_success.pdf",
  plot = p_general,
  device = "pdf",
  path = path,
  width = 8,
  height = 5
)

# ================================
# 1.2. Outlier Analysis
# ================================

### (a) Summary of total flips and success by person
person_summary <- df %>%
  group_by(person) %>%
  summarize(
    total_flips = sum(m),
    mean_success = round(mean(success, na.rm = TRUE), 3),
    .groups = "drop"
  ) %>%
  arrange(mean_success)

print(person_summary)

### (a) Summary of total flips and success by coin
coin_summary <- df %>%
  group_by(coin) %>%
  summarize(
    total_flips = sum(m),
    mean_success = round(mean(success, na.rm = TRUE), 3),
    .groups = "drop"
  ) %>%
  arrange(mean_success)

print(coin_summary)

# Summary statistics by person and coin combination
person_coin_summary <- df %>%
  group_by(person, coin) %>%
  summarize(
    total_flips = sum(m),
    mean_success = round(mean(success, na.rm = TRUE), 3),
    .groups = "drop"
  ) %>%
  arrange(mean_success)

print(person_coin_summary)

# Prepare violin plot data
violin_data <- data.frame(
  category = factor(c(rep("by person", nrow(person_summary)),
                      rep("by coin", nrow(coin_summary)),
                      rep("by person x coin", nrow(person_coin_summary)))),
  mean_success = c(person_summary$mean_success,
                   coin_summary$mean_success,
                   person_coin_summary$mean_success)
)

# Create violin plot
p_violin <- ggplot(violin_data, aes(x = "", y = mean_success)) +
  geom_violin(fill = "steelblue", color = "white", alpha = 0.8) +
  geom_boxplot(width = 0.1, fill = "lightgrey", color = "black", outlier.color = "red") +
  facet_wrap(~ category) +
  labs(
    title = "Distribution of Success Proportions",
    x = "",
    y = "Success Proportion"
  ) +
  theme_bw() +
  theme(
    plot.title = element_text(hjust = 0.5),
    plot.caption = element_text(hjust = 0.5),
    legend.title = element_text(hjust = 0.5),
    legend.text = element_text(hjust = 0.5),
    strip.background = element_rect(fill = "grey90"),
    strip.text = element_text(face = "bold")
  )

print(p_violin)

# Save the violin plot
ggsave(
  filename = "violin_plot_success.pdf",
  plot = p_violin,
  device = "pdf",
  path = path,
  width = 8,
  height = 5
)

# ================================
# 1.3. Temporal Effects
# ================================

# Transform the ACF results in DataFrame
acf_result <- acf(df_time$mean_same_side, plot = FALSE)
acf_df <- data.frame(
  lag = acf_result$lag,
  acf = acf_result$acf
)

# Create ACF plot
p_acf <- ggplot(acf_df, aes(x = lag, y = acf)) +
  geom_bar(stat = "identity", fill = "steelblue", color = "white", alpha = 0.8) +
  geom_hline(yintercept = 0, color = "black") +
  geom_hline(yintercept = c(-1.96 / sqrt(nrow(df_time)), 1.96 / sqrt(nrow(df_time))), linetype = "dashed", color = "red") +
  labs(
    title = "Autocorrelation Function (ACF) of the Success Proportion",
    x = "Lag",
    y = "ACF"
  ) +
  theme_bw() +
  theme(
    plot.title = element_text(hjust = 0.5),
    legend.title = element_text(hjust = 0.5),
    legend.text = element_text(hjust = 0.5),
    strip.background = element_rect(fill = "grey90"),
    strip.text = element_text(face = "bold")
  )

print(p_acf)

# Save the ACF plot
ggsave(
  filename = "acf_success.pdf",
  plot = p_acf,
  device = "pdf",
  path = path,
  width = 8,
  height = 5
)



## 2. GLM Binomial

### 2.1. Base model

```{r}
#| label: "GLM Binomial base"
#| echo: false
#| message: false 
#| warning: false
#| error: false

# Adjustment of the simple GLM binomial model
base_glm <- glm(cbind(y, m - y) ~ 1, family = binomial, data = df)

# Summary of the model to see the results 
summary(base_glm)

(ci_logit_base <- confint(base_glm, level = 0.95))

# Transformation of logit in order to obtain a Confidence Interval
ic_p_base <- exp(ci_logit_base) / (1 + exp(ci_logit_base))
cat("\n95% CI for p (base_glm): [",
    round(ic_p_base[1], 4), ", ",
    round(ic_p_base[2], 4), "]\n\n")

base_coef <- coef(base_glm)
p_initial <- exp(base_coef) / (1 + exp(base_coef))
cat("Estimated same-side probability (initial model):", round(p_initial, 4), "\n")


# Computation of the dispersion ratio
dispersion_ratio <- summary(base_glm)$deviance / summary(base_glm)$df.residual
cat("Dispersion Ratio:", dispersion_ratio, "\n")

# Preparing the data for the residuals plot
df_resid <- data.frame(
  fitted = fitted(base_glm),
  pearson_res = residuals(base_glm, type = "pearson")
)


# Residuals plot
p_residuals <- ggplot(df_resid, aes(x = fitted, y = pearson_res)) +
  geom_point(color = "steelblue", alpha = 0.7) +
  geom_hline(yintercept = 0, color = "red", linetype = "dashed") +
  labs(
    title = "Résidus vs Valeurs Prévues",
    x = "Valeurs Prévues",
    y = "Résidus Pearson"
  ) +
  theme_bw() +
  theme(
    plot.title = element_text(hjust = 0.5),
    plot.caption = element_text(hjust = 0.5)
  )

print(p_residuals)


df_resid$pearson_res <- residuals(base_glm, type = "pearson")

p_qq <- ggplot(df_resid, aes(sample = pearson_res)) +
  stat_qq(color = "steelblue", alpha = 0.7) +
  stat_qq_line(color = "red", linetype = "dashed") +
  labs(
    title = "Q-Q Plot des Résidus Pearson",
    x = "Quantiles théoriques",
    y = "Quantiles des résidus"
  ) +
  theme_bw() +
  theme(
    plot.title = element_text(hjust = 0.5),
    plot.caption = element_text(hjust = 0.5)
  )

print(p_qq)

ggsave(
  filename = "qq_residuals.pdf",  
  plot = p_qq,  
  device = "pdf", 
  path = path, 
  width = 8,
  height = 5
)


# Compute the quantiles and IQR (Inter Quartil Range)
Q1 <- quantile(person_coin_summary$mean_success, 0.25)
Q3 <- quantile(person_coin_summary$mean_success, 0.75)
IQR_value <- Q3 - Q1

# Define the bounds for outliers
lower_bound <- Q1 - 1.5 * IQR_value
upper_bound <- Q3 + 1.5 * IQR_value

# Filter the observation detected as outliers
outliers_pc <- person_coin_summary %>%
  filter(mean_success < lower_bound | mean_success > upper_bound)

print(outliers_pc)

# Compute the Cook's distance for all the observations
cooks_d <- cooks.distance(base_glm)

# Add Cooks distance to the initial data 
df_with_cook <- df
df_with_cook$cooks_distance <- cooks_d

# Extract the person-coin from the ouliers
outlier_groups <- outliers_pc %>% select(person, coin)

# Filtrer les données originales pour les observations appartenant à ces groupes
df_outliers <- df_with_cook %>%
  inner_join(outlier_groups, by = c("person", "coin"))

# Examiner les distances de Cook pour ces observations
df_outliers_selected <- df_outliers %>% select(person, coin, y, m, cooks_distance)

print(df_outliers_selected)

n <- nrow(df)

print(threshold <- 8/(n - 2*1))
# = 0.01904  ## So some values exceed this value. Then try refiting the model without these values

# Identify the observations with a Cook's distance > threshold
influential_indices <- which(cooks_d > threshold)

# Erase these observations from the initial data 
df_cleaned <- df[-influential_indices, ]

# Refit a new simple binomial GLM on this new dataset - withtout the detected outliers
refit_glm <- glm(cbind(y, m - y) ~ 1, family = binomial, data = df_cleaned)

# Print a summary of the new model
summary(refit_glm)

(ci_logit_refit <- confint(refit_glm, level = 0.95))
ic_p_refit <- exp(ci_logit_refit) / (1 + exp(ci_logit_refit))
cat("\n95% CI for p (refit_glm): [",
    round(ic_p_refit[1], 4), ", ",
    round(ic_p_refit[2], 4), "]\n\n")

refit_coef <- coef(refit_glm)
p_refit <- exp(refit_coef) / (1 + exp(refit_coef))
cat("Estimated same-side probability (model without influential points):", round(p_refit, 4), "\n")

```

### 2.2. Normal model

```{r}
#| label: "Normal linear model base"
#| echo: false
#| message: false 
#| warning: false
#| error: false

# Create the proportion column by dividing 'y' by 'm'
df$prop <- df$y / df$m

# Fit a weighted least squares (WLS) model
wls_model <- lm(prop ~ 1, data = df, weights = df$m)

# ============================#
# 2) Parameter extraction
# ============================#


beta0_hat <- coef(wls_model)[1]              # Estimate of the intercept
var_beta0_hat <- vcov(wls_model)[1, 1]       # Variance of the intercept
se_beta0_hat <- sqrt(var_beta0_hat)          # Standard error

# ============================#
# 3) Hypothesis Test H0: beta0 = 0.5
# ============================#

# Calculate t-value for the hypothesis test
T_value <- (beta0_hat - 0.5) / se_beta0_hat
df_res   <- wls_model$df.residual

# Bilateral p-value (since the alternative hypothesis is two-sided : beta0 != 0.5)
p_value <- 2 * pt(abs(T_value), df_res, lower.tail = FALSE)

# Display the test results
cat("\nTest H0: beta0 = 0.5 vs. H1: beta0 != 0.5 (WLS model)\n")
cat("---------------------------------------\n")
cat("Estimated beta0      =", round(beta0_hat, 4), "\n")
cat("Std Error           =", round(se_beta0_hat, 4), "\n")
cat("T-value             =", round(T_value, 4), "\n")
cat("Degrees of freedom  =", df_res, "\n")
cat("p-value (bilateral) =", format.pval(p_value, digits=4), "\n\n")

# Calculate the 95% confidence interval for beta0
alpha <- 0.05
t_crit <- qt(1 - alpha / 2, df_res)  # t quantile for degrees of freedom

# Calculate the margin of error for the confidence interval
delta <- t_crit * se_beta0_hat
center <- beta0_hat
IC_left  <- center - delta
IC_right <- center + delta

# Display the confidence interval
cat("Manual 95% CI (around the estimate, testing 0.5) : [",
    round(IC_left, 4), ", ", round(IC_right, 4), "]\n\n")

```

### 2.3. Fixed effect GLM

```{r}
#| label: "GLM Binomial"
#| echo: false
#| message: false 
#| warning: false
#| error: false

# Define formulas for different candidate models
formulas <- list(
  "start + person + coin" = cbind(y, m - y) ~ start + person + coin,
  "start + person"       = cbind(y, m - y) ~ start + person,
  "start + coin"         = cbind(y, m - y) ~ start + coin,
  "person + coin"        = cbind(y, m - y) ~ person + coin,
  "start"                = cbind(y, m - y) ~ start,
  "person"               = cbind(y, m - y) ~ person,
  "coin"                 = cbind(y, m - y) ~ coin,
  "intercept"            = cbind(y, m - y) ~ 1
)

# Initialize lists to store models and their AIC values
model_list <- list()
aic_values <- numeric(length(formulas))

# Fit each model and calculate its AIC value
for(i in seq_along(formulas)) {
  formula_i <- formulas[[i]]
  model_list[[i]] <- glm(formula_i, family = binomial, data = df)
  aic_values[i] <- AIC(model_list[[i]])
}

# Create a summary table of models and their AIC values
model_comparison <- data.frame(
  Model = names(formulas),
  AIC = aic_values
)

# Sort the table by AIC in ascending order
model_comparison <- model_comparison[order(model_comparison$AIC), ]
print(model_comparison)

# Identify the best model based on the minimum AIC value
best_model_index <- which.min(aic_values)
best_model_name <- model_comparison$Model[1]
best_model <- model_list[[best_model_index]]

cat("Best model according to AIC:", best_model_name, "\n")
summary(best_model)

# Assume that the best model is the one with the effect of 'person' only
best_model <- glm(cbind(y, m - y) ~ person, family = binomial, data = df)
summary(best_model)

# Calculate Pearson residuals for the optimal model
residuals_best <- residuals(best_model, type = "pearson")
fitted_best <- fitted(best_model)

# Prepare a data frame for residuals visualization
df_resid_best <- data.frame(fitted = fitted_best, pearson_res = residuals_best)

# Plot residuals vs predicted values
ggplot(df_resid_best, aes(x = fitted, y = pearson_res)) +
  geom_point(color = "steelblue", alpha = 0.7) +
  geom_hline(yintercept = 0, color = "red", linetype = "dashed") +
  labs(title = "Residuals vs Predicted Values (Optimal Model)",
       x = "Predicted Values", y = "Pearson Residuals") +
  theme_bw()

# Calculate the dispersion ratio for the optimal model
dispersion_ratio <- summary(best_model)$deviance / summary(best_model)$df.residual
cat("Dispersion Ratio:", dispersion_ratio, "\n")

# Fit a quasi-binomial GLM 
quasi_glm <- glm(cbind(y, m - y) ~ person, family = quasibinomial, data = df)
summary(quasi_glm)
```

### 2.4. Random Effect GLM

```{r}
#| label: "Random Effect GLM"
#| echo: false
#| message: false 
#| warning: false
#| error: false

# Fit a random effect GLM with 'person' as the random effect
random_effect_person <- glmer(
  cbind(y, m - y) ~ 1 + (1 | person), 
  family = binomial, 
  data = df
)

# Display the summary of the random effect model
summary(random_effect_person)

# Calculate the dispersion ratio for the random effect model
dispersion_ratio <- summary(random_effect_person)$deviance / summary(random_effect_person)$df.residual
cat("Dispersion Ratio:", dispersion_ratio, "\n")
```

### 2.5. Nested Effect GLM

```{r}
#| label: "Nested Effect GLM"
#| echo: false
#| message: false 
#| warning: false
#| error: false

# Fit a nested random effect GLM with 'person' and 'person:coin' as random effects
re_model_nested <- glmer(
  cbind(y, m - y) ~ 1 + (1 | person) + (1 | person:coin),
  family = binomial,
  data = df
)

# Display the summary and AIC of the nested model
summary(re_model_nested)
AIC(re_model_nested)

# Compare the random effect model with the nested model using likelihood ratio test (LRT)
anova(random_effect_person, re_model_nested, test = "LRT")
AIC(random_effect_person, re_model_nested)

# Calculate the dispersion ratio for the nested effect model
dispersion_ratio <- summary(re_model_nested)$deviance / summary(re_model_nested)$df.residual
cat("Dispersion Ratio:", dispersion_ratio, "\n")

```
